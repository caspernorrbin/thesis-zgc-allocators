\subsection{Integration Into ZGC} \label{sec:futureworkZ}

The most significant future step is to integrate the allocators into ZGC. As highlighted in Section~\ref{sec:individual_contrubitons}, Gärds~\cite{niclas} has concurrently to this thesis explored the integration of an allocator into ZGC, focusing on a free-list-based allocator to enhance the efficiency of relocating objects within ZGC. Future efforts could build on Gärds' work by incorporating a buddy allocator into ZGC or exploring alternative integration scenarios not considered by Gärds.

When integrating a buddy allocator specifically, the garbage collector can implement strategies to mitigate some of its drawbacks. The primary issue is the considerable internal fragmentation during allocation. To minimize this, ZGC could allocate multiple objects simultaneously to align more closely with size requirements or use fragmented memory from an allocation for future use. This approach requires that objects allocated together be deallocated collectively. However, as detailed in Section~\ref{sec:freerangeexpl}, this is not an issue when deallocating ranges.

\subsection{Smaller Minimum Allocation Size} \label{sec:futureworkLiliput}
Currently, the minimum allocation size in ZGC is 16 bytes, constrained by the header size of Java objects. Efforts are underway to decrease the size of these headers, potentially lowering the smallest object size to 8 bytes \cite{liliput}. Since this development is ongoing, it was not considered in this thesis.

Reducing the minimum allocation size would introduce a new bottom layer for the allocators. Practically, this results in memory overhead per page doubling as it becomes necessary to manage and track smaller allocations. Additionally, for the binary buddy and iBuddy allocators, storing the two pointers needed for the free-list would no longer be feasible. These pointers would need to be transformed into offsets from the beginning of the page. Although the size of the pages makes this adjustment manageable, it would still introduce some performance overhead.

\newpage
\subsection{Further Allocation Optimizations} \label{sec:futureworkOptimizations}
Improving the binary tree storage format in the binary tree allocator might be feasible. Currently, the levels of the tree are stored sequentially, requiring progressively larger jumps when iterating over the levels. To increase spatial locality, the larger tree could be broken down into several subtrees stored consecutively. This would place the segments closer together, reducing the likelihood of cache misses and potentially increasing performance.

Currently, all the adapted allocators use the same concurrency approach, which involves dividing memory into regions and only permitting concurrent allocations in different regions. However, since their designs differ, it would be worth investigating different designs for each allocator. For example, a lock-free mechanism could achieve concurrency for the binary tree allocator. As the binary tree allocator navigates its tree structure, compare-and-swap (CAS) atomic operations could replace the current subtraction operations. Conflicts could arise with other threads, but threads can back off to resolve these conflicts. Consequently, it is advisable to keep distributing allocations evenly using regions, but with the locking code removed, thereby only allocating concurrently within the same region when necessary.

% alla samma, dåligt
% undersöka anpassing för specifika versioner
% binary tree extra bra!!! därför att......
% 

\subsection{Combining Allocation Strategies} \label{sec:futureworkCombine}
Another area of future research could involve exploring hybrid approaches that combine the strengths of various buddy allocator designs. This concept parallels the strategies used in other allocators like the linux kernel~\cite{linuxbuddy} and \texttt{jemalloc}~\cite{jemalloc}, which combine multiple allocation techniques to optimize performance and memory utilization.

For instance, a hybrid buddy allocator could employ a binary tree allocator for large block sizes and an iBuddy allocator for small block sizes, thereby leveraging the advantages of both approaches. These allocators could be distributed across different memory regions, optimizing their performance based on specific memory requirements. Alternatively, one could investigate methods to enable these allocators to work in tandem on the same memory region, dynamically switching between allocation strategies as needed to maximize efficiency and minimize fragmentation.
