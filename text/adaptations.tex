As discussed in Section~\ref{sec:buddyadditions}, there is ample room to improve the original binary buddy allocator. The two designs discussed have different strengths, so both are implemented to compare and contrast with each other and the original design. The binary tree allocator closely resembles the binary buddy allocator in its allocation method, whereas the iBuddy allocator differs significantly. Both allocators are investigated to determine the most efficient allocation and deallocation processes for all possible allocatable sizes.

Further enhancements can be developed that are orthogonal to the specific allocator implementations. These enhancements can be general optimizations or specifically tailored to the context of operating within ZGC. Constraining the use case enables the allocators to assume access to additional information or to narrow their functionality. This section discusses the adaptations made to the allocators to improve their performance within ZGC.

\subsection{Allocator Configurability} \label{sec:adaptationsconfig}
There are numerous scenarios in which a garbage collector could benefit from an advanced memory allocator, many of which are beyond the scope of this thesis. Each scenario has distinct characteristics and unique requirements. Improvements should enhance specific scenarios without causing adverse effects in other situations.
If a modification improves one use case but negatively impacts another, it should be possible to disable that change. This flexibility allows each scenario to utilize the most suitable configuration for that specific case by enabling or disabling the desired features. The adaptations made are indepentant of one another, allowing for easy configuration changes of specific features.

Making the allocator configurable improves its applicability to a wide range of scenarios. Although this versatility is advantageous, it may lead to limitations in certain instances. Prioritizing configurability can hinder the allocator from being fully optimized for one particular use case. Significant modifications to enhance one specific scenario could adversely affect others. The choice was made to prioritize configurability instead of restricting the allocator to a single use case. This decision allows the allocator to be used in a variety of scenarios, with the user selecting the most suitable configuration for their specific needs.

\subsection{Allocator Metadata Implementations} \label{sec:adaptationsmetadata}
For some scenarios, a specific metadata implementation may be more beneficial than others. The metadata structures can be altered to either prioritize memory efficiency or speed, depending on needs. This section covers the choices made regarding the metadata storage of the allocators.

\subsubsection{Page Metadata} \label{sec:metadataexpl}

The design of the buddy allocator requires storing additional metadata separate from the data on the page. This metadata, which stores the status of possible blocks, incurs a fixed overhead for each page allocated, rather than increasing with each allocation. The size of the metadata is influenced by the total number of possible blocks rather than their size. Consequently, having larger block sizes lead less of a proportional overhead, whereas smaller block sizes result in significantly higher proportional overhead.

The basic binary buddy allocator is the most efficient in terms of memory usage, requiring the least data. The state of each potential block must be stored, which can be represented by a single bit. This data is only used during deallocation, so the states of two buddy blocks can be merged into one bit using XOR operations, effectively halving the memory requirement. This optimization is not possible for the iBuddy allocator, as blocks can be deallocated without knowing the state of their buddy.

% \subsubsection{Binary Tree Metadata} \label{sec:binarytreeexpl}
The binary tree allocator retains the design of the original binary buddy allocator but stores its metadata in a different format. This approach allows for faster allocation and deallocation, as there is no explicit free-list that needs to be managed.

The binary tree is stored as a flattened byte array, with each level stored contiguously following the preceding one. Most levels have each byte representing the value of a single block, but the lowest levels deviate from this pattern. Each node in the tree holds the maximum possible level that can be allocated within it or its children. The smallest blocks can only store $1$ or $0$, enabling memory optimization by compacting and storing 8 blocks within each byte in the array. Optimizing this level, which contains half of the total blocks, results in memory savings of $43.75$\%. Similarly, the data for the next two levels can be compacted to 2 bits per block, further reducing memory by $28.13$\%. Higher levels require at least $4$ bits of information and constitute a smaller fraction of total blocks, so their memory is not optimized to avoid performance impacts from bit operations.

\subsubsection{Finding Block Buddies} \label{sec:findbuddiesexpl}
When deallocating, the allocator must determine the correct block to deallocate based on the memory address. Since an address can belong to any block level, additional information is required. The simplest solution is to require the user to provide the allocated size, allowing the allocator to calculate the deallocation level. This method is fast and incurs minimal overhead as the allocator does not need to store additional data. However, it requires the user to keep track of the allocated size, which may not always be feasible.

Alternatively, the level of each allocation can be stored inside the block or in a separate data structure. Although this is simple and allows for fast lookups, it is not very memory-efficient. If the data is stored within a block, the usable space decreases. Storing data separately for each possible block location results in a slightly larger but constant overhead.

A third approach uses a bitmap to track which blocks have been split. This method is very memory-efficient but requires more operations to find the correct blocks. Figure~\ref{fig:buddybmapsplit} illustrates this bitmap after a 16-byte block allocation, showing all blocks above the allocated block marked as split. The smallest blocks do not require an entry in the bitmap, as they cannot be split. During deallocation, the allocator starts at the top of the bitmap and traverses down until the block is no longer marked as split.

\begin{figure}[h]
    \centering
    \includesvg[width=0.64\textwidth]{figures/bbuddy_bmap_split.svg}
    \caption{The state of the split blocks-bitmap and free-list of a binary buddy allocator after one 16-byte allocation.}
    \label{fig:buddybmapsplit}
\end{figure}

All the three options are implemented and can be configured in the allocators, with the most suitable option depending on the specific usage scenario. For general use, the bitmap approach is the most memory-efficient across various allocator configurations.

For use within ZGC, data structures responsible for tracking size can be omitted, delegating this function to the garbage collector. The collector has sufficient data from its live analysis and object headers to compute the size of each object and allocation. This approach eliminates additional overhead, making optimal use of existing resources.

\subsection{Using the Allocators on Already Allocated Memory} \label{sec:freerangeexpl}
In certain situations, using an advanced allocator to manage memory may not be the most efficient option. For instance, bump-pointer allocation could be used initially, switching to a more advanced allocator when significant external fragmentation arises. This approach maximizes the speed advantage of bump-pointer allocations while leveraging advanced allocators when necessary.

In the context of ZGC, the allocator must be able to allocate around memory already in use. This imposes limitations on the allocator, as it no longer ``owns'' the memory it is allocating from. The allocator instead has to allocate in the holes of available memory. This affects where the allocator metadata can be stored, as it is commonly stored within its own memory region.

When working with memory not fully controlled by the allocator, the allocator starts with no available memory for allocation. The user must indicate the locations of occupied memory or the intervals between them, defining free regions that the allocator can use. In ZGC, live analysis of a page provides information on live objects, enabling deallocation of the intervals between them.

When working with previously allocated memory, the allocator starts fully occupied, with no available locations for allocation. The user must indicate the locations of occupied memory or the intervals between them, defining free regions that the allocator can use. In ZGC, live analysis of a page provides information on live objects, enabling deallocation of the intervals between them.
This imposes limitations on the allocator, preventing it from assuming that memory is available for its utilization. This affects where the allocator metadata can be stored, as it is commonly stored within its own memory region.

When working with previously allocated memory, the allocator starts fully occupied, with no available locations for allocation. The user must indicate the locations of occupied memory or the intervals between them, defining free regions that the allocator can use. In ZGC, live analysis of a page provides information on live objects, enabling deallocation of the intervals between them.

To implement functionality for this scenario, the allocator's initial state is fully occupied with the smallest-sized blocks, avoiding accidental merging of blocks overlapping with occupied regions. During the deallocation of a specific range, the largest fitting blocks are added to the free-list, and each block and its split children are marked as no longer split. Figure~\ref{fig:deallocrange} illustrates this process, showing blocks added to the free-list covering the entire free range.

\begin{figure}[h]
    \centering
    \includesvg[width=0.62\textwidth]{figures/deallocrange.svg}
    \caption{The blocks added to the free-list after deallocating the range up until the last block.}
    \label{fig:deallocrange}
\end{figure}

\subsection{Lazy Splitting and Merging of Blocks} \label{sec:lazyexpl}
Splitting and merging blocks is costly and should be avoided if possible. The binary buddy allocator merges blocks whenever possible, often requiring immediate splitting for subsequent allocations. Lee and Barkley \cite{lazylayer} designed a modified buddy allocator that delays block merging. As long as the allocation size distribution remains consistent, costs related to splitting and merging can be avoided. A simpler version of Lee and Barkley's design is implemented, with a second free-list (lazy layer) on top of the buddy allocator (buddy layer). This separate free-list does not perform any splitting or merging and solely stores blocks of different sizes.

When deallocating using a lazy layer, the block is inserted into the lazy layer's free-list. If the lazy layer reaches a certain threshold of blocks, the block is inserted into the buddy layer as normal. During allocation, the lazy layer is checked first for a suitable block, followed by splitting blocks from the buddy layer if necessary. If both steps fail, the lazy layer can be emptied to allow smaller blocks to be merged to meet the required allocation size.

It is logical for different block sizes to have different thresholds, as the frequency of sizes differs. Since most allocations are small, it is also logical that smaller blocks should have a higher threshold. Storing large blocks in the lazy layer would also reserve these large blocks for only that size, which could lead to smaller allocations not being able to be fulfilled. When implementing this, the smallest block size has the highest threshold, with each level above that halving the threshold of the previous level. The default threshold is set to 1000. A high value is desired, as ZGC can often deallocate many objects to then allocate many objects of the same size. The lazy layer needs the capacity to support this, which is why the threshold is set relatively high.

Different block sizes have different thresholds, with the smallest block size having the highest threshold. Large blocks in the lazy layer could prevent smaller allocations from being fulfilled, so thresholds decrease for larger block sizes. The default threshold is set to 1000 for the smallest block size, with the threshold halving for each level above that. A high threshold is desired, as ZGC could deallocate many objects to afterward allocate many objects of the same size. The lazy layer needs the capacity to support this, which is why the threshold is set relatively high.

\subsection{Allocator Regions} \label{sec:concurrencyexpl}
In the original binary buddy allocator, the largest block equals the entire provided memory, which is then split into smaller blocks during allocation. This approach can degrade performance in multithreaded programs with concurrent allocations, as only one allocation can split a large block at a time.

To avoid this, the maximum block size is set lower than the entire memory size, dividing the memory into smaller regions, each with a normal buddy block structure. Park et al.~\cite{park2014ibuddy} suggest this for the iBuddy allocator. For example, splitting memory into two regions, each with a maximum block size of half the total memory, allows for two concurrent allocations. If one region is prioritized, fragmentation would be reduced as one region would fulfill smaller allocations, leaving space for larger allocations in the second region.

In ZGC, the maximum allocation size in a small page is $256$ KiB, a fraction of the page size of 2 MiB. This allows for eight allocations of the maximum size within a page. The allocator can be divided into eight regions, each covering one-eighth of a page. Fewer regions would limit the number of concurrent allocations, while more regions would restrict the maximum allocation size.

One allocator controlls all regions, keeping shared metadata such as the lazy layer, while each region has its own buddy structure. This design allows for concurrent allocations in different regions, as long as the allocations are evenly distributed. The scalability extends up to the number of regions, with additional allocations needing to wait for prior allocations to complete.

Situations could arise where some regions become more heavily utilized than others, decreasing throughput. When a thread initiates an allocation, it is assigned a specific region. If another thread is allocating in that region, it checks the subsequent regions. If all regions are in use, the thread waits. Upon entering a region, the necessary allocation logic is performed. If an allocation fails due to insufficient space, the thread exits the region and retries in the next one. Allocation requests fail when all regions cannot accommodate the request.

\subsection{Putting it All Together} \label{sec:adaptationsall}
All the mentioned adaptations build on top of the three different buddy allocators discussed in Sections~\ref{sec:buddy} and \ref{sec:buddyadditions}. Each adaptation can be enabled or disabled independently, allowing for a wide range of configurations. The user can select the most suitable configuration for their specific use case, optimizing the allocator for their needs. The adaptations are designed to enhance the allocator's performance within ZGC, but can improve the allocator's performance in other scenarios as well.

To configure the allocator, the user first selects which base allocator to use. The binary buddy allocator is the most memory-efficient, the binary tree allocator is a faster alternative at the cost of memory, and the iBuddy allocator is designed for fast small block allocations. The user can then configure the adaptations, selecting the minimum and maximum block sizes, the number of regions, the metadata storage method, and the lazy layer thresholds. The allocator is then ready for use, optimized for the user's specific needs.